services:
  # Layer 1: Client Access Layer
  nginx:
    image: nginx:alpine
    container_name: nginx
    volumes:
      #- ./configs/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./configs/nginx/conf.d:/etc/nginx/conf.d:ro
      - ./configs/nginx/ssl:/etc/nginx/ssl:ro
      - ./configs/nginx/html:/usr/share/nginx/html:ro
    ports:
      - "80:80"
      - "443:443"
    networks:
      - frontend
      - security
    depends_on:
      kong:
        condition: service_healthy
      init-kong:
        condition: service_completed_successfully
      keycloak:
        condition: service_healthy
    command: >
      /bin/sh -c '
      echo "Waiting for dependencies..." &&
      sleep 30 &&
      echo "Starting Nginx..." &&
      nginx-debug -g "daemon off;"
      '
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Layer 2: Security Layer
  init-keycloak-db:
    image: postgres:15-alpine
    environment:
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: postgres
    networks:
      - database
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      /bin/sh -c '
      echo "Creating keycloak database..." &&
      psql -v ON_ERROR_STOP=1 --username "${POSTGRES_USER}" --host postgres --dbname postgres --command "CREATE DATABASE keycloak WITH OWNER ${POSTGRES_USER};" &&
      echo "Granting privileges..." &&
      psql -v ON_ERROR_STOP=1 --username "${POSTGRES_USER}" --host postgres --dbname postgres --command "GRANT ALL PRIVILEGES ON DATABASE keycloak TO ${POSTGRES_USER};"
      '
    restart: on-failure

  keycloak:
    image: quay.io/keycloak/keycloak:latest
    container_name: keycloak
    command: start-dev
    environment:
      - KEYCLOAK_ADMIN=admin
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD}
      - KC_DB=postgres
      - KC_DB_URL=jdbc:postgresql://postgres:5432/keycloak
      - KC_DB_USERNAME=${POSTGRES_USER}
      - KC_DB_PASSWORD=${POSTGRES_PASSWORD}
      - KC_HOSTNAME=localhost
      - KC_HTTP_PORT=8180
      - KC_PROXY=edge
      - KC_HEALTH_ENABLED=true
    ports:
      - "8180:8180"
    volumes:
      - ./configs/keycloak:/opt/keycloak/data/import
    networks:
      - security
      - database
    depends_on:
      init-keycloak-db:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8180/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  kong-database:
    image: postgres:15-alpine
    container_name: kong-database
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-kong}
      POSTGRES_DB: ${POSTGRES_DB:-kong}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-kongpass}
    command: postgres -c 'max_connections=500'
    volumes:
      - kong_data:/var/lib/postgresql/data
    networks:
      - security
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "kong"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  kong-migrations:
    image: kong:latest
    container_name: kong-migrations
    command: sh -c "kong migrations bootstrap && kong migrations up"
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_USER: ${POSTGRES_USER:-kong}
      KONG_PG_PASSWORD: ${POSTGRES_PASSWORD:-kongpass}
      KONG_PG_DATABASE: ${POSTGRES_DB:-kong}
    networks:
      - security
    depends_on:
      kong-database:
        condition: service_healthy
    restart: on-failure

  kong:
    image: kong:latest
    container_name: kong
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_USER: ${POSTGRES_USER:-kong}
      KONG_PG_PASSWORD: ${POSTGRES_PASSWORD:-kongpass}
      KONG_PG_DATABASE: ${POSTGRES_DB:-kong}
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_PROXY_LISTEN: 0.0.0.0:8000
      KONG_PROXY_LISTEN_SSL: 0.0.0.0:8443
      KONG_ADMIN_GUI_URL: http://localhost:8002
      KONG_ADMIN_GUI_LISTEN: 0.0.0.0:8002
      KONG_ADMIN_GUI_PATH: /
      KONG_ADMIN_GUI_AUTH: basic-auth
      KONG_ADMIN_GUI_SESSION_CONF: '{"secret":"your-secret-key","storage":"kong","cookie_secure":false}'
      KONG_ENFORCE_RBAC: "off"
      KONG_ADMIN_GUI_AUTH_CONF: '{"credential_names":["admin"],"header_names":["apikey"]}'
    depends_on:
      kong-migrations:
        condition: service_completed_successfully
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
      - "8443:8443"
    networks:
      - security
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Layer 3: Data Processing Layer
  validator:
    build:
      context: ./validator
      dockerfile: Dockerfile
    container_name: validator
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/validatordb
      - SPRING_DATASOURCE_USERNAME=${POSTGRES_USER}
      - SPRING_DATASOURCE_PASSWORD=${POSTGRES_PASSWORD}
      - SPRING_JPA_HIBERNATE_DDL_AUTO=update
      - JAVA_OPTS=-Xmx512m -Xms256m
      - SERVER_PORT=8080
    ports:
      - "8080:8080"
    networks:
      - processing
      - database
    depends_on:
      init-validatordb:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    restart: unless-stopped

  init-validatordb:
    image: postgres:15-alpine
    environment:
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: postgres
    networks:
      - database
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      /bin/sh -c '
      echo "Creating validatordb database..." &&
      psql -v ON_ERROR_STOP=1 --username "${POSTGRES_USER}" --host postgres --dbname postgres --command "CREATE DATABASE validatordb WITH OWNER ${POSTGRES_USER};" &&
      echo "Granting privileges..." &&
      psql -v ON_ERROR_STOP=1 --username "${POSTGRES_USER}" --host postgres --dbname postgres --command "GRANT ALL PRIVILEGES ON DATABASE validatordb TO ${POSTGRES_USER};"
      '
    restart: on-failure

  # Layer 4: Task Orchestration Layer
  task-manager:
    image: openjdk:17-slim
    container_name: task-manager
    working_dir: /app
    volumes:
      - ./task-manager:/app
    command: >
      sh -c '
        echo "Waiting for dependencies..." &&
        sleep 15 &&
        java -jar task-manager.jar
      '
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/taskdb
      - SPRING_DATASOURCE_USERNAME=${POSTGRES_USER}
      - SPRING_DATASOURCE_PASSWORD=${POSTGRES_PASSWORD}
      - SPRING_JPA_HIBERNATE_DDL_AUTO=update
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SERVER_PORT=8084
    ports:
      - "8084:8084"
    networks:
      - orchestration
      - database
      - cache
      - queue
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8084/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Layer 5: Queue Management Layer
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    networks:
      - queue
    healthcheck:
      test: echo srvr | nc localhost 2181 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - queue
    healthcheck:
      test: kafka-topics --bootstrap-server=localhost:9092 --list || exit 1
      interval: 30s
      timeout: 10s
      retries: 3

  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
    networks:
      - queue

  # Layer 6: Resource Management Layer
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./configs/postgresql:/etc/postgresql/conf.d
    networks:
      - database
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    command: postgres -c 'max_connections=500'

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
      - ./configs/redis/redis.conf:/usr/local/etc/redis/redis.conf
    networks:
      - cache

  # Layer 7: LLM Engine Processing Layer
  llm-engine:
    build:
      context: ./llm-engine
      dockerfile: Dockerfile
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
    volumes:
      - ./llm-engine:/app
    networks:
      - processing
    depends_on:
      - redis
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      mode: replicated
      replicas: 3
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first

  # Layer 8: Result Processing Layer
  spark:
    image: apache/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8181
    ports:
      - "8181:8181"  # Web UI
      - "7077:7077"  # Spark master port
    volumes:
      - ./spark/conf:/opt/spark/conf
      - ./spark/data:/opt/spark/data
    networks:
      - processing
    healthcheck:
      test: nc -z localhost 7077 || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  spark-worker:
    image: apache/spark:latest
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_PORT=8182
      - SPARK_WORKER_WEBUI_PORT=8183
    ports:
      - "8182:8182"
      - "8183:8183"
    volumes:
      - ./spark/conf:/opt/spark/conf
      - ./spark/data:/opt/spark/data
    networks:
      - processing
    depends_on:
      - spark
    restart: unless-stopped

  # Layer 9: Monitoring Layer
  prometheus:
    image: prom/prometheus
    volumes:
      - ./configs/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    networks:
      - monitoring

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - monitoring
    depends_on:
      - prometheus

  # Layer 10: Reliability Layer
  minio:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    networks:
      - storage

  init-kong:
    image: curlimages/curl
    depends_on:
      kong:
        condition: service_healthy
    environment:
      VALIDATOR_URL: ${VALIDATOR_URL:-http://validator:8080}
      TASK_MANAGER_URL: ${TASK_MANAGER_URL:-http://task-manager:8084}
      LLM_ENGINE_URL: ${LLM_ENGINE_URL:-http://llm-engine:5000}
    command: |
      /bin/sh -c '
      set -e
      '
    networks:
      - security
    restart: on-failure

networks:
  frontend:
  security:
  processing:
  orchestration:
  queue:
  database:
  cache:
  monitoring:
  storage:

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:
  minio_data:
  zookeeper_data:
  zookeeper_log:
  kafka_data:
  kong_data: